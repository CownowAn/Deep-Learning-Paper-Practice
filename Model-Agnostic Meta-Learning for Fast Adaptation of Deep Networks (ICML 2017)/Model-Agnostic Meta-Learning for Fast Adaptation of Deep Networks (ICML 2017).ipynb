{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (ICML 2017).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNcFZ6IVRnBV37uemzi9wqz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"15e8e5eb6df24b59a00434bc7104487f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_13a6871db355495780662a59adb57dc1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3b8300e556fb4b42a6def36ba0765f64","IPY_MODEL_3e78bb4fb781497d9e677310d8b49887","IPY_MODEL_fc6f3f7128a54309bb2b3b93e8826110"]}},"13a6871db355495780662a59adb57dc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b8300e556fb4b42a6def36ba0765f64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_52eb62115096488cb9080c318124ae7d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_58be75dab6a24e1ebbcc1569a8b03d29"}},"3e78bb4fb781497d9e677310d8b49887":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_65e926d7e4504a53ac2d2dcb36362374","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9464212,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9464212,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3abc2ff4f9f94a538b8816c007c09072"}},"fc6f3f7128a54309bb2b3b93e8826110":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_db2205350e0740b5accd2b8b909dc5f6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9464832/? [00:00&lt;00:00, 52089297.17it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_733536baefd0449e92dc8552b25ed0a3"}},"52eb62115096488cb9080c318124ae7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"58be75dab6a24e1ebbcc1569a8b03d29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"65e926d7e4504a53ac2d2dcb36362374":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3abc2ff4f9f94a538b8816c007c09072":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db2205350e0740b5accd2b8b909dc5f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"733536baefd0449e92dc8552b25ed0a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cdc14453bb234f089bd8998281521e2c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c14eb6645db1449f9e0f3dba94b6f22c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7b39296e82ff480e80c54d7444e25cf8","IPY_MODEL_878dec453e844afdbca0f735da9b2218","IPY_MODEL_01e4da4a56c447d3a96792693fe7de00"]}},"c14eb6645db1449f9e0f3dba94b6f22c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b39296e82ff480e80c54d7444e25cf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6c8047b069af44afa95acce4bdd772fc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_32628722e1a644eeafee019dafbfb43e"}},"878dec453e844afdbca0f735da9b2218":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_05556d6a80c0485391e3917404467a28","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":6462886,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6462886,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bf18ac3dfd8a4fd399d241074a34f971"}},"01e4da4a56c447d3a96792693fe7de00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_999c3b08344e43d6b68ceb789b7a1b93","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 6463488/? [00:00&lt;00:00, 14612354.58it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bf833a60665044f5a7c1c808cbfa4754"}},"6c8047b069af44afa95acce4bdd772fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"32628722e1a644eeafee019dafbfb43e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05556d6a80c0485391e3917404467a28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bf18ac3dfd8a4fd399d241074a34f971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"999c3b08344e43d6b68ceb789b7a1b93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bf833a60665044f5a7c1c808cbfa4754":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","source":["# Omniglot dataset check"],"metadata":{"id":"saMkLaLqIum5"}},{"cell_type":"code","source":["!pip install --upgrade virtualenv\n","!virtualenv venv\n","!source venv/bin/activate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LVUxQ17YFHd-","executionInfo":{"status":"ok","timestamp":1646399239642,"user_tz":-540,"elapsed":16384,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"36a3a831-718f-4ea7-ef10-9f4f4d4af712"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting virtualenv\n","  Downloading virtualenv-20.13.2-py2.py3-none-any.whl (8.7 MB)\n","\u001b[K     |████████████████████████████████| 8.7 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (3.6.0)\n","Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (4.11.2)\n","Collecting distlib<1,>=0.3.1\n","  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n","\u001b[K     |████████████████████████████████| 461 kB 38.2 MB/s \n","\u001b[?25hCollecting platformdirs<3,>=2\n","  Downloading platformdirs-2.5.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.7.0)\n","Installing collected packages: platformdirs, distlib, virtualenv\n","Successfully installed distlib-0.3.4 platformdirs-2.5.1 virtualenv-20.13.2\n","created virtual environment CPython3.7.12.final.0-64 in 1241ms\n","  creator CPython3Posix(dest=/content/venv, clear=False, no_vcs_ignore=False, global=False)\n","  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n","    added seed packages: pip==22.0.3, setuptools==60.9.3, wheel==0.37.1\n","  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"]}]},{"cell_type":"code","source":["!pip install torchmeta"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Tfzi0npFHgi","executionInfo":{"status":"ok","timestamp":1646399350743,"user_tz":-540,"elapsed":111105,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"f5ea6c8a-ca39-4634-c0f3-77617788756a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmeta\n","  Downloading torchmeta-1.8.0-py3-none-any.whl (210 kB)\n","\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 194 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 204 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 210 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from torchmeta) (3.1.0)\n","Collecting torchvision<0.11.0,>=0.5.0\n","  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n","\u001b[K     |████████████████████████████████| 22.1 MB 65.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchmeta) (2.23.0)\n","Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (7.1.2)\n","Collecting torch<1.10.0,>=1.4.0\n","  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 7.1 kB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (4.63.0)\n","Collecting ordered-set\n","  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (1.21.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.10.0,>=1.4.0->torchmeta) (3.10.0.2)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->torchmeta) (1.5.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (3.0.4)\n","Installing collected packages: torch, torchvision, ordered-set, torchmeta\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.10.0+cu111\n","    Uninstalling torch-1.10.0+cu111:\n","      Successfully uninstalled torch-1.10.0+cu111\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.11.1+cu111\n","    Uninstalling torchvision-0.11.1+cu111:\n","      Successfully uninstalled torchvision-0.11.1+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n","Successfully installed ordered-set-4.1.0 torch-1.9.1 torchmeta-1.8.0 torchvision-0.10.1\n"]}]},{"cell_type":"code","source":["!pip install torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l196bRHeFi1A","executionInfo":{"status":"ok","timestamp":1646399354517,"user_tz":-540,"elapsed":3783,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"ff860e23-cb83-424c-ecd2-35592f45dc43"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.1)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: torch==1.9.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1->torchvision) (3.10.0.2)\n"]}]},{"cell_type":"code","source":["from torchmeta.datasets import Omniglot\n","from torchmeta.transforms import Categorical, ClassSplitter, Rotation\n","from torchvision.transforms import Compose, Resize, ToTensor\n","from torchmeta.utils.data import BatchMetaDataLoader"],"metadata":{"id":"jEkwdzNaZKy-","executionInfo":{"status":"ok","timestamp":1646399355873,"user_tz":-540,"elapsed":1364,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["dataset = Omniglot(\"data\",\n","                   num_classes_per_task=5,\n","                   transform=Compose([Resize(28), ToTensor()]),\n","                   target_transform=Categorical(num_classes=5),\n","                   class_augmentations=[Rotation([90, 180, 270])],\n","                   meta_train=True,\n","                   download=True)"],"metadata":{"id":"hUl8BLQLZK1U","colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["15e8e5eb6df24b59a00434bc7104487f","13a6871db355495780662a59adb57dc1","3b8300e556fb4b42a6def36ba0765f64","3e78bb4fb781497d9e677310d8b49887","fc6f3f7128a54309bb2b3b93e8826110","52eb62115096488cb9080c318124ae7d","58be75dab6a24e1ebbcc1569a8b03d29","65e926d7e4504a53ac2d2dcb36362374","3abc2ff4f9f94a538b8816c007c09072","db2205350e0740b5accd2b8b909dc5f6","733536baefd0449e92dc8552b25ed0a3","cdc14453bb234f089bd8998281521e2c","c14eb6645db1449f9e0f3dba94b6f22c","7b39296e82ff480e80c54d7444e25cf8","878dec453e844afdbca0f735da9b2218","01e4da4a56c447d3a96792693fe7de00","6c8047b069af44afa95acce4bdd772fc","32628722e1a644eeafee019dafbfb43e","05556d6a80c0485391e3917404467a28","bf18ac3dfd8a4fd399d241074a34f971","999c3b08344e43d6b68ceb789b7a1b93","bf833a60665044f5a7c1c808cbfa4754"]},"executionInfo":{"status":"ok","timestamp":1646399386084,"user_tz":-540,"elapsed":30221,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"0ffaf7e7-df5e-4774-ec82-822c919a7fe4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip to data/omniglot/images_background.zip\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15e8e5eb6df24b59a00434bc7104487f","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/9464212 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip to data/omniglot/images_evaluation.zip\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdc14453bb234f089bd8998281521e2c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/6462886 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"code","source":["dataset = ClassSplitter(dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n","dataloader = BatchMetaDataLoader(dataset, batch_size=16, num_workers=4)"],"metadata":{"id":"qo8TjGyUZK3q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646399386085,"user_tz":-540,"elapsed":24,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"599efed8-1d70-427c-9eac-2eb1463521dc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["for i, batch in enumerate(dataloader):\n","  if i == 10: break\n","  train_inputs, train_targets = batch['train']\n","  test_inputs, test_targets = batch['test']\n","  print('------'*10)\n","  print('Batch ID: {0}'.format(i))\n","  print('Train inputs shape per batch: {0}'.format(train_inputs.shape))\n","  print('Train targets shape per batch: {0}'.format(train_targets.shape))\n","  print()\n","  print('Test inputs shape per batch: {0}'.format(test_inputs.shape))\n","  print('Test targets shape per batch: {0}'.format(test_targets.shape))"],"metadata":{"id":"2JIg6dTGZK6A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646399397526,"user_tz":-540,"elapsed":11462,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"2e980f52-1aaf-4ee5-89ff-d907a1a1fe0e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------\n","Batch ID: 0\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 1\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 2\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 3\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 4\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 5\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 6\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 7\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 8\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 9\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n"]}]},{"cell_type":"markdown","source":["# Model architecture for Omniglot dataset"],"metadata":{"id":"MLCCrG_dI1YL"}},{"cell_type":"code","source":["import torch.nn as nn\n","from collections import OrderedDict\n","from torchmeta.modules import (MetaModule, MetaConv2d, MetaBatchNorm2d, MetaSequential, MetaLinear)"],"metadata":{"id":"R_X3TKJ0HZYC","executionInfo":{"status":"ok","timestamp":1646399397526,"user_tz":-540,"elapsed":4,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def conv_block(in_channels, out_channels, **kwargs):\n","  return MetaSequential(OrderedDict([\n","                                     ('conv', MetaConv2d(in_channels, out_channels, **kwargs)),\n","                                     ('norm', MetaBatchNorm2d(out_channels, momentum=1., track_running_stats=False)),\n","                                     ('relu', nn.ReLU()),\n","                                     ('pool', nn.MaxPool2d(2))\n","  ]))"],"metadata":{"id":"5_Rs7McLJRjD","executionInfo":{"status":"ok","timestamp":1646399397527,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class MetaConvModel(MetaModule):\n","  def __init__(self, in_channels, num_ways, hidden_size=64, feature_size=64):\n","    super(MetaConvModel, self).__init__()\n","    self.in_channels = in_channels\n","    self.num_ways = num_ways\n","    self.hidden_size = hidden_size,\n","    self.feature_size = feature_size\n","    self.features = MetaSequential(OrderedDict([\n","                                                ('layer1', conv_block(in_channels, hidden_size, kernel_size=3, stride=1, padding=1, bias=True)),\n","                                                ('layer2', conv_block(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1, bias=True)),\n","                                                ('layer3', conv_block(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1, bias=True)),\n","                                                ('layer4', conv_block(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1, bias=True))\n","    ]))\n","    self.classifier = MetaLinear(feature_size, num_ways, bias=True)\n","\n","  def forward(self, inputs, params=None):\n","    features = self.features(inputs, params=self.get_subdict(params, 'features'))\n","    features = features.view((features.size(0), -1))\n","    logits = self.classifier(features, params=self.get_subdict(params, 'classifier'))\n","    return logits"],"metadata":{"id":"XPQSsyncJRlX","executionInfo":{"status":"ok","timestamp":1646399397527,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def ModelConvOmniglot(num_ways, hidden_size=64):\n","  return MetaConvModel(1, num_ways, hidden_size=hidden_size, feature_size=hidden_size)"],"metadata":{"id":"KzbI0bRFJRnt","executionInfo":{"status":"ok","timestamp":1646399424158,"user_tz":-540,"elapsed":662,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# Meta Learners"],"metadata":{"id":"l9oApdi0PA8m"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from tqdm import tqdm\n","from torchmeta.utils import gradient_update_parameters"],"metadata":{"id":"0hY-V6D4Pfdf","executionInfo":{"status":"ok","timestamp":1646399397527,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy(logits, targets):\n","  with torch.no_grad():\n","    _, predictions = torch.max(logits, dim=1)\n","    accuracy = torch.mean(predictions.eq(targets).float())\n","  return accuracy.item()"],"metadata":{"id":"SCSzX5azXmJe","executionInfo":{"status":"ok","timestamp":1646399397527,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def tensors_to_device(tensors, device=torch.device('cpu')):\n","  if isinstance(tensors, torch.Tensor):\n","    return tensors.to(device=device)\n","  elif isinstance(tensors, (list, tuple)):\n","    return type(tensors)(tensors_to_device(tensor, device=device) for tensor in tensors)\n","  elif isinstance(tensors, (dict, OrderedDict)):\n","    return type(tensors)([(key, tensors_to_device(tensor, device=device)) for (key, tensor) in tensors.items()])\n","  else:\n","    raise NotImplementedError()"],"metadata":{"id":"f395vEVgvRGX","executionInfo":{"status":"ok","timestamp":1646399397527,"user_tz":-540,"elapsed":4,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class MAML(object):\n","  def __init__(self, model, optimizer=None, step_size=0.1, num_adaptation_steps=1, scheduler=None, loss_func=F.cross_entropy, device=None):\n","    self.model = model\n","    self.optimizer = optimizer\n","    self.step_size = step_size\n","    self.num_adaptation_steps = num_adaptation_steps\n","    self.scheduler = scheduler\n","    self.loss_func = loss_func\n","    self.device = device\n","\n","    if scheduler is not None:\n","      for group in self.optimizer.param_groups:\n","        group.setdefault('initial_lr', group['lr'])\n","      self.scheduler.base_lrs([group['initial_lr'] for group in self.optimizer.param_groups])\n","\n","  def adapt(self, inputs, targets, num_adaptation_steps=1, step_size=0.1):\n","    params = None\n","    # Initialize results['inner losses'] (per task)\n","    results = {'inner_losses': np.zeros((num_adaptation_steps, ), dtype=np.float32)}\n","    for step in range(num_adaptation_steps):\n","      logits = self.model(inputs, params=params)\n","      inner_loss = self.loss_func(logits, targets)\n","      results['inner_losses'][step] = inner_loss.item()\n","      if step == 0:\n","        results['accuracy_before'] = compute_accuracy(logits, targets)\n","      self.model.zero_grad()\n","      params = gradient_update_parameters(self.model, inner_loss, step_size=step_size, params=params)\n","    return params, results\n","\n","  # per batch\n","  def get_outer_loss(self, batch):\n","    if 'test' not in batch:\n","      raise RuntimeError('The batch does not contain any test dataset')\n","\n","    _,  test_targets = batch['test']\n","    num_tasks = test_targets.size(0) # Here, 16\n","\n","    # Initialize results\n","    results = {\n","        'num_tasks': num_tasks,\n","        'inner_losses': np.zeros((self.num_adaptation_steps, num_tasks), dtype=np.float32),\n","        'outer_losses': np.zeros((num_tasks, ), dtype=np.float32),\n","        'mean_outer_loss': 0.,\n","        'accuracies_before': np.zeros((num_tasks, ), dtype=np.float32), # Before adaptation\n","        'accuracies_after': np.zeros((num_tasks, ), dtype=np.float32) # After adaptation\n","    }\n","\n","    mean_outer_loss = torch.tensor(0., device=self.device)\n","\n","    for task_id, (train_inputs, train_targets, test_inputs, test_targets) in enumerate(zip(*batch['train'], *batch['test'])):\n","      # adaptation_results(per task): {'inner_losses': (num_adaptation_steps, ), 'accuracy_before': compute_accuracy(logits, targets)} \n","      params, adaptation_results = self.adapt(train_inputs, train_targets, \n","                                              num_adaptation_steps=self.num_adaptation_steps, step_size=self.step_size)\n","      results['inner_losses'][:, task_id] = adaptation_results['inner_losses']\n","      results['accuracies_before'][task_id] = adaptation_results['accuracy_before']\n","      with torch.set_grad_enabled(self.model.training):\n","        test_logits = self.model(test_inputs, params=params) # param is updated with (train_inputs, train_targets)\n","        outer_loss = self.loss_func(test_logits, test_targets)\n","        results['outer_losses'][task_id] = outer_loss.item()\n","        mean_outer_loss += outer_loss\n","      results['accuracies_after'][task_id] = compute_accuracy(test_logits, test_targets)\n","    \n","    mean_outer_loss.div_(num_tasks)\n","    results['mean_outer_loss'] = mean_outer_loss.item()\n","\n","    return mean_outer_loss, results\n","\n","  def train_iter(self, dataloader, max_batches=500):\n","    num_batches = 0\n","    self.model.train()\n","    model.to(device=self.device)\n","    while num_batches < max_batches:\n","      for batch in dataloader:\n","        if num_batches >= max_batches: break\n","\n","        if self.scheduler is not None:\n","          self.scheduler.step(epoch=num_batches)\n","\n","        self.optimizer.zero_grad()\n","        batch = tensors_to_device(batch, device=self.device)\n","        outer_loss, results = self.get_outer_loss(batch)\n","        yield results\n","\n","        outer_loss.backward()\n","        self.optimizer.step()\n","\n","        num_batches += 1\n","\n","  def train(self, dataloader, max_batches=500, verbose=True, **kwargs):\n","    with tqdm(total=max_batches, disable=not verbose, **kwargs) as pbar:\n","      for results in self.train_iter(dataloader, max_batches=max_batches):\n","        pbar.update(1)\n","        postfix = {'loss': '{0:.4f}'.format(results['mean_outer_loss'])}\n","        if 'accuracies_after' in results:\n","          postfix['accuracy'] = '{0:.4f}'.format(np.mean(results['accuracies_after']))\n","        pbar.set_postfix(**postfix)\n","  \n","  def evaluate_iter(self, dataloader, max_batches=500):\n","    num_batches = 0\n","    self.model.eval()\n","    model.to(device=self.device)\n","    while num_batches < max_batches:\n","      for batch in dataloader:\n","        if num_batches >= max_batches: break\n","\n","        batch = tensors_to_device(batch, device=self.device)\n","        _, results = self.get_outer_loss(batch)\n","        yield results\n","\n","        num_batches += 1\n","  \n","  def evaluate(self, dataloader, max_batches=500, verbose=True, **kwargs):\n","    mean_outer_loss, mean_accuracy, count = 0., 0., 0\n","    with tqdm(total=max_batches, disable=not verbose, **kwargs) as pbar:\n","      for results in self.evaluate_iter(dataloader, max_batches=max_batches):\n","        pbar.update(1)\n","        count += 1\n","        mean_outer_loss += (results['mean_outer_loss'] - mean_outer_loss) / count # Running average for the batches\n","        postfix = {'loss': '{0:.4f}'.format(mean_outer_loss)}\n","        if 'accuracies_after' in results:\n","          mean_accuracy += (np.mean(results['accuracies_after']) - mean_accuracy) / count # Running average for the batches\n","          postfix['accuracy'] = '{0:.4f}'.format(mean_accuracy)\n","        pbar.set_postfix(**postfix)\n","    \n","    mean_results = {'mean_outer_loss': mean_outer_loss}\n","    if 'accuracies_after' in results:\n","      mean_results['accuracies_after'] = mean_accuracy\n","      \n","    return mean_results"],"metadata":{"id":"saimerPyNV53","executionInfo":{"status":"ok","timestamp":1646401607460,"user_tz":-540,"elapsed":386,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"k4Sp4UbO21WR"}},{"cell_type":"code","source":["import torch\n","import math\n","import os\n","import time\n","import json\n","import logging"],"metadata":{"id":"q0RzjEMI28FR","executionInfo":{"status":"ok","timestamp":1646401610776,"user_tz":-540,"elapsed":1,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxYBh92M28HU","executionInfo":{"status":"ok","timestamp":1646401611718,"user_tz":-540,"elapsed":510,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"597c7ee8-531f-4619-eb58-ef8c379ed523"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["output_folder = '/path/to/data'\n","if not os.path.exists(output_folder):\n","  os.makedirs(output_folder)\n","  logging.debug('Creating output_folder `{0}`'.format(output_folder))\n","\n","  folder = os.path.join(output_folder, time.strftime('%Y-%m-%d_%H%M%S'))\n","  os.makedirs(folder)\n","  logging.debug('Creating folder `{0}`'.format(folder))\n","\n","  folder = os.path.abspath(folder)\n","  model_path = os.path.abspath(os.path.join(folder, 'model.th'))\n","\n","# For Omniglot dataset\n","batch_size=16\n","num_workers=4\n","\n","meta_train_dataset = Omniglot(root=\"data\",\n","                  num_classes_per_task=5,\n","                  transform=Compose([Resize(28), ToTensor()]),\n","                  target_transform=Categorical(num_classes=5),\n","                  class_augmentations=[Rotation([90, 180, 270])],\n","                  meta_train=True,\n","                  download=True)\n","meta_train_dataset = ClassSplitter(meta_train_dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n","meta_train_dataloader = BatchMetaDataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n","\n","meta_val_dataset = Omniglot(root=\"data\",\n","                  num_classes_per_task=5,\n","                  transform=Compose([Resize(28), ToTensor()]),\n","                  target_transform=Categorical(num_classes=5),\n","                  class_augmentations=[Rotation([90, 180, 270])],\n","                  meta_val=True,\n","                  download=True)\n","meta_val_dataset = ClassSplitter(meta_train_dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n","meta_val_dataloader = BatchMetaDataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n","\n","meta_train_dataset = Omniglot(root=\"data\",\n","                  num_classes_per_task=5,\n","                  transform=Compose([Resize(28), ToTensor()]),\n","                  target_transform=Categorical(num_classes=5),\n","                  class_augmentations=[Rotation([90, 180, 270])],\n","                  meta_train=True,\n","                  download=True)\n","meta_train_dataset = ClassSplitter(meta_train_dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n","meta_train_dataloader = BatchMetaDataLoader(dataset, batch_size=batch_size, num_workers=num_workers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5YdkVTFt28NT","executionInfo":{"status":"ok","timestamp":1646401617469,"user_tz":-540,"elapsed":431,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"4f3e471e-a9c3-407e-afe3-795c6ddac295"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["model = ModelConvOmniglot(num_ways=5, hidden_size=64)\n","num_batches=100\n","loss_func = F.cross_entropy\n","meta_lr = 0.001\n","meta_optimizer = torch.optim.Adam(model.parameters(), lr=meta_lr)\n","metalearner = MAML(model=model, optimizer=meta_optimizer, step_size=0.1, num_adaptation_steps=1, \n","                   scheduler=None, loss_func=loss_func, device=device)"],"metadata":{"id":"TD29fjZw6i4c","executionInfo":{"status":"ok","timestamp":1646401620035,"user_tz":-540,"elapsed":513,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["model.to(device)\n","best_value = None\n","\n","num_epochs = 50\n","for epoch in range(num_epochs):\n","  metalearner.train(dataloader=meta_train_dataloader, max_batches=num_batches, verbose=True)\n","  results = metalearner.evaluate(dataloader=meta_val_dataloader, max_batches=num_batches, verbose=True)\n","  \n","  if 'accuracies_after' in results:\n","    if (best_value is None) or (best_value < results['accuracies_after']):\n","      best_value = results['accuracies_after']\n","      save_model = True\n","  elif (best_value is None) or (best_value > results['mean_outer_loss']):\n","    best_value = results['mean_outer_loss']\n","    save_model = True\n","  else:\n","    save_model = False\n","\n","  if save_model and (output_folder is not None):\n","    with open(model_path, 'wb') as f:\n","      torch.save(model.state_dict(), f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKh7MXnC9Oyc","outputId":"1f6e1345-f9b7-4098-8c64-69cf9e817b55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.03s/it, accuracy=0.9742, loss=0.1229]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:25<00:00,  1.16it/s, accuracy=0.9690, loss=0.1371]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.03s/it, accuracy=0.9858, loss=0.0675]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:16<00:00,  1.30it/s, accuracy=0.9782, loss=0.0860]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.02s/it, accuracy=0.9808, loss=0.0597]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:18<00:00,  1.28it/s, accuracy=0.9871, loss=0.0548]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.02s/it, accuracy=0.9883, loss=0.0462]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:18<00:00,  1.27it/s, accuracy=0.9892, loss=0.0440]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.02s/it, accuracy=0.9933, loss=0.0306]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.28it/s, accuracy=0.9886, loss=0.0436]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.02s/it, accuracy=0.9875, loss=0.0456]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:16<00:00,  1.30it/s, accuracy=0.9907, loss=0.0376]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.02s/it, accuracy=0.9925, loss=0.0293]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.29it/s, accuracy=0.9918, loss=0.0326]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.02s/it, accuracy=0.9942, loss=0.0298]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.30it/s, accuracy=0.9931, loss=0.0280]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.02s/it, accuracy=0.9933, loss=0.0257]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.30it/s, accuracy=0.9913, loss=0.0315]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.01s/it, accuracy=0.9933, loss=0.0245]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.28it/s, accuracy=0.9943, loss=0.0213]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.02s/it, accuracy=0.9958, loss=0.0139]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:18<00:00,  1.28it/s, accuracy=0.9943, loss=0.0217]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.02s/it, accuracy=0.9867, loss=0.0423]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:16<00:00,  1.30it/s, accuracy=0.9909, loss=0.0324]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.01s/it, accuracy=0.9833, loss=0.0476]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.30it/s, accuracy=0.9925, loss=0.0270]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:40<00:00,  1.01s/it, accuracy=0.9967, loss=0.0171]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:16<00:00,  1.30it/s, accuracy=0.9939, loss=0.0229]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.03s/it, accuracy=0.9875, loss=0.0344]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:18<00:00,  1.28it/s, accuracy=0.9938, loss=0.0232]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.03s/it, accuracy=0.9875, loss=0.0290]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"," 25%|██▌       | 25/100 [00:20<01:09,  1.07it/s, accuracy=0.9949, loss=0.0177]"]}]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"2IL8ybPWJ8bq"}},{"cell_type":"code","source":["with open(model_path, 'rb') as f:\n","  best_model = torch.load(f, map_location=device)"],"metadata":{"id":"uhimu8aQJ90K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"liQSMGsbJ92e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yTW3bZZmJ941"},"execution_count":null,"outputs":[]}]}
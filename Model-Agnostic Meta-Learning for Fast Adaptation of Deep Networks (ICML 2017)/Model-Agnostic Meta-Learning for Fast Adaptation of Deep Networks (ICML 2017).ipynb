{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (ICML 2017).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNjA5Vp+BB/0x9rhDwn4J/U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9fff255b59f44078aabf0afc0799bfcf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9951367a1012434f92b6c732dd2964d9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ca256f9766fe48d6a6674b8999fb058b","IPY_MODEL_513da46136b34c309b66aa072d334200","IPY_MODEL_7a4f53d4a68f4e59b16768086cb7b353"]}},"9951367a1012434f92b6c732dd2964d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ca256f9766fe48d6a6674b8999fb058b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a92b7eb6237a452abc9ca51f3991eb2f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3fb809f98347499fb25868120367117f"}},"513da46136b34c309b66aa072d334200":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5499da6b37f144c7a8d6eefdaaf08519","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9464212,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9464212,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9c08830623894ff78e0d935d35d74022"}},"7a4f53d4a68f4e59b16768086cb7b353":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ce3aa1461e74863b7b3152448076ea0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9464832/? [00:00&lt;00:00, 46310035.74it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_55fb0d2044cd42f5be2fc30ae6b0fb32"}},"a92b7eb6237a452abc9ca51f3991eb2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3fb809f98347499fb25868120367117f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5499da6b37f144c7a8d6eefdaaf08519":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9c08830623894ff78e0d935d35d74022":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ce3aa1461e74863b7b3152448076ea0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"55fb0d2044cd42f5be2fc30ae6b0fb32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d0a33767641a472f87dd690ba1417dca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_14d171e200544763abbff7bcda07c7e7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f9c96510a9ff453f99e5c5f736b38fe5","IPY_MODEL_9fd2349acd4841f09fb9618d6a2fece5","IPY_MODEL_2411a44cd0534118ba69c6cf3be119cf"]}},"14d171e200544763abbff7bcda07c7e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f9c96510a9ff453f99e5c5f736b38fe5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e8494d72888242b2873f6040fb32f20f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_221dcda6143e49819b4ab42d6b6f4a87"}},"9fd2349acd4841f09fb9618d6a2fece5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0d0b493dac0c46bcb5542015fb398863","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":6462886,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6462886,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4eb4ce20db6c46469aaa14f75bdd90ea"}},"2411a44cd0534118ba69c6cf3be119cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_83d7ab5075d3483f9db251cbfe2d21f7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 6463488/? [00:00&lt;00:00, 13660088.05it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_984f4658a7c34073b79343dcbf329df9"}},"e8494d72888242b2873f6040fb32f20f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"221dcda6143e49819b4ab42d6b6f4a87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d0b493dac0c46bcb5542015fb398863":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4eb4ce20db6c46469aaa14f75bdd90ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"83d7ab5075d3483f9db251cbfe2d21f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"984f4658a7c34073b79343dcbf329df9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","source":["# Omniglot dataset check"],"metadata":{"id":"saMkLaLqIum5"}},{"cell_type":"code","source":["!pip install --upgrade virtualenv\n","!virtualenv venv\n","!source venv/bin/activate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LVUxQ17YFHd-","executionInfo":{"status":"ok","timestamp":1646446723901,"user_tz":-540,"elapsed":17677,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"3c4e5d53-c5e5-492d-aca7-274c321059ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting virtualenv\n","  Downloading virtualenv-20.13.2-py2.py3-none-any.whl (8.7 MB)\n","\u001b[K     |████████████████████████████████| 8.7 MB 12.2 MB/s \n","\u001b[?25hRequirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (1.15.0)\n","Collecting distlib<1,>=0.3.1\n","  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n","\u001b[K     |████████████████████████████████| 461 kB 39.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (3.6.0)\n","Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (4.11.2)\n","Collecting platformdirs<3,>=2\n","  Downloading platformdirs-2.5.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.7.0)\n","Installing collected packages: platformdirs, distlib, virtualenv\n","Successfully installed distlib-0.3.4 platformdirs-2.5.1 virtualenv-20.13.2\n","created virtual environment CPython3.7.12.final.0-64 in 1632ms\n","  creator CPython3Posix(dest=/content/venv, clear=False, no_vcs_ignore=False, global=False)\n","  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n","    added seed packages: pip==22.0.3, setuptools==60.9.3, wheel==0.37.1\n","  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"]}]},{"cell_type":"code","source":["!pip install torchmeta"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Tfzi0npFHgi","executionInfo":{"status":"ok","timestamp":1646446836070,"user_tz":-540,"elapsed":112175,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"a87c2643-3c36-4d77-81b6-300bcb645f93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmeta\n","  Downloading torchmeta-1.8.0-py3-none-any.whl (210 kB)\n","\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 174 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 194 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 204 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 210 kB 13.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchmeta) (2.23.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (1.21.5)\n","Collecting torch<1.10.0,>=1.4.0\n","  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 6.7 kB/s \n","\u001b[?25hRequirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (7.1.2)\n","Collecting ordered-set\n","  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (4.63.0)\n","Collecting torchvision<0.11.0,>=0.5.0\n","  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n","\u001b[K     |████████████████████████████████| 22.1 MB 64.1 MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from torchmeta) (3.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.10.0,>=1.4.0->torchmeta) (3.10.0.2)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->torchmeta) (1.5.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (3.0.4)\n","Installing collected packages: torch, torchvision, ordered-set, torchmeta\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.10.0+cu111\n","    Uninstalling torch-1.10.0+cu111:\n","      Successfully uninstalled torch-1.10.0+cu111\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.11.1+cu111\n","    Uninstalling torchvision-0.11.1+cu111:\n","      Successfully uninstalled torchvision-0.11.1+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n","Successfully installed ordered-set-4.1.0 torch-1.9.1 torchmeta-1.8.0 torchvision-0.10.1\n"]}]},{"cell_type":"code","source":["!pip install torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l196bRHeFi1A","executionInfo":{"status":"ok","timestamp":1646446840058,"user_tz":-540,"elapsed":3996,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"550be57f-3e93-4b80-d597-20e8f18a3f15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.1)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n","Requirement already satisfied: torch==1.9.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1->torchvision) (3.10.0.2)\n"]}]},{"cell_type":"code","source":["from torchmeta.datasets import Omniglot\n","from torchmeta.transforms import Categorical, ClassSplitter, Rotation\n","from torchvision.transforms import Compose, Resize, ToTensor\n","from torchmeta.utils.data import BatchMetaDataLoader"],"metadata":{"id":"jEkwdzNaZKy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = Omniglot(\"data\",\n","                   num_classes_per_task=5,\n","                   transform=Compose([Resize(28), ToTensor()]),\n","                   target_transform=Categorical(num_classes=5),\n","                   class_augmentations=[Rotation([90, 180, 270])],\n","                   meta_train=True,\n","                   download=True)"],"metadata":{"id":"hUl8BLQLZK1U","colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["9fff255b59f44078aabf0afc0799bfcf","9951367a1012434f92b6c732dd2964d9","ca256f9766fe48d6a6674b8999fb058b","513da46136b34c309b66aa072d334200","7a4f53d4a68f4e59b16768086cb7b353","a92b7eb6237a452abc9ca51f3991eb2f","3fb809f98347499fb25868120367117f","5499da6b37f144c7a8d6eefdaaf08519","9c08830623894ff78e0d935d35d74022","3ce3aa1461e74863b7b3152448076ea0","55fb0d2044cd42f5be2fc30ae6b0fb32","d0a33767641a472f87dd690ba1417dca","14d171e200544763abbff7bcda07c7e7","f9c96510a9ff453f99e5c5f736b38fe5","9fd2349acd4841f09fb9618d6a2fece5","2411a44cd0534118ba69c6cf3be119cf","e8494d72888242b2873f6040fb32f20f","221dcda6143e49819b4ab42d6b6f4a87","0d0b493dac0c46bcb5542015fb398863","4eb4ce20db6c46469aaa14f75bdd90ea","83d7ab5075d3483f9db251cbfe2d21f7","984f4658a7c34073b79343dcbf329df9"]},"executionInfo":{"status":"ok","timestamp":1646446870429,"user_tz":-540,"elapsed":29903,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"e71888ea-aa8a-43e0-b78d-5dec820c2679"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip to data/omniglot/images_background.zip\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fff255b59f44078aabf0afc0799bfcf","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/9464212 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip to data/omniglot/images_evaluation.zip\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0a33767641a472f87dd690ba1417dca","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/6462886 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"code","source":["dataset = ClassSplitter(dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n","dataloader = BatchMetaDataLoader(dataset, batch_size=16, num_workers=4)"],"metadata":{"id":"qo8TjGyUZK3q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646446870430,"user_tz":-540,"elapsed":39,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"c2fc0d00-65c8-4788-9678-39af5a03f9ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["for i, batch in enumerate(dataloader):\n","  if i == 10: break\n","  train_inputs, train_targets = batch['train']\n","  test_inputs, test_targets = batch['test']\n","  print('------'*10)\n","  print('Batch ID: {0}'.format(i))\n","  print('Train inputs shape per batch: {0}'.format(train_inputs.shape))\n","  print('Train targets shape per batch: {0}'.format(train_targets.shape))\n","  print()\n","  print('Test inputs shape per batch: {0}'.format(test_inputs.shape))\n","  print('Test targets shape per batch: {0}'.format(test_targets.shape))"],"metadata":{"id":"2JIg6dTGZK6A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646446882731,"user_tz":-540,"elapsed":12332,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"165702af-a257-4045-8c63-b0c7115c0a9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------\n","Batch ID: 0\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 1\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 2\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 3\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 4\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 5\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 6\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 7\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 8\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n","------------------------------------------------------------\n","Batch ID: 9\n","Train inputs shape per batch: torch.Size([16, 25, 1, 28, 28])\n","Train targets shape per batch: torch.Size([16, 25])\n","\n","Test inputs shape per batch: torch.Size([16, 75, 1, 28, 28])\n","Test targets shape per batch: torch.Size([16, 75])\n"]}]},{"cell_type":"markdown","source":["# Model architecture for Omniglot dataset"],"metadata":{"id":"MLCCrG_dI1YL"}},{"cell_type":"code","source":["import torch.nn as nn\n","from collections import OrderedDict\n","from torchmeta.modules import (MetaModule, MetaConv2d, MetaBatchNorm2d, MetaSequential, MetaLinear)"],"metadata":{"id":"R_X3TKJ0HZYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def conv_block(in_channels, out_channels, **kwargs):\n","  return MetaSequential(OrderedDict([\n","                                     ('conv', MetaConv2d(in_channels, out_channels, **kwargs)),\n","                                     ('norm', MetaBatchNorm2d(out_channels, momentum=1., track_running_stats=False)),\n","                                     ('relu', nn.ReLU()),\n","                                     ('pool', nn.MaxPool2d(2))\n","  ]))"],"metadata":{"id":"5_Rs7McLJRjD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MetaConvModel(MetaModule):\n","  def __init__(self, in_channels, num_ways, hidden_size=64, feature_size=64):\n","    super(MetaConvModel, self).__init__()\n","    self.in_channels = in_channels\n","    self.num_ways = num_ways\n","    self.hidden_size = hidden_size,\n","    self.feature_size = feature_size\n","    self.features = MetaSequential(OrderedDict([\n","                                                ('layer1', conv_block(in_channels, hidden_size, kernel_size=3, stride=1, padding=1, bias=True)),\n","                                                ('layer2', conv_block(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1, bias=True)),\n","                                                ('layer3', conv_block(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1, bias=True)),\n","                                                ('layer4', conv_block(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1, bias=True))\n","    ]))\n","    self.classifier = MetaLinear(feature_size, num_ways, bias=True)\n","\n","  def forward(self, inputs, params=None):\n","    features = self.features(inputs, params=self.get_subdict(params, 'features'))\n","    features = features.view((features.size(0), -1))\n","    logits = self.classifier(features, params=self.get_subdict(params, 'classifier'))\n","    return logits"],"metadata":{"id":"XPQSsyncJRlX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ModelConvOmniglot(num_ways, hidden_size=64):\n","  return MetaConvModel(1, num_ways, hidden_size=hidden_size, feature_size=hidden_size)"],"metadata":{"id":"KzbI0bRFJRnt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Meta Learners"],"metadata":{"id":"l9oApdi0PA8m"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from tqdm import tqdm\n","from torchmeta.utils import gradient_update_parameters"],"metadata":{"id":"0hY-V6D4Pfdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy(logits, targets):\n","  with torch.no_grad():\n","    _, predictions = torch.max(logits, dim=1)\n","    accuracy = torch.mean(predictions.eq(targets).float())\n","  return accuracy.item()"],"metadata":{"id":"SCSzX5azXmJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tensors_to_device(tensors, device=torch.device('cpu')):\n","  if isinstance(tensors, torch.Tensor):\n","    return tensors.to(device=device)\n","  elif isinstance(tensors, (list, tuple)):\n","    return type(tensors)(tensors_to_device(tensor, device=device) for tensor in tensors)\n","  elif isinstance(tensors, (dict, OrderedDict)):\n","    return type(tensors)([(key, tensors_to_device(tensor, device=device)) for (key, tensor) in tensors.items()])\n","  else:\n","    raise NotImplementedError()"],"metadata":{"id":"f395vEVgvRGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MAML(object):\n","  def __init__(self, model, optimizer=None, step_size=0.1, num_adaptation_steps=1, scheduler=None, loss_func=F.cross_entropy, device=None):\n","    self.model = model\n","    self.optimizer = optimizer\n","    self.step_size = step_size\n","    self.num_adaptation_steps = num_adaptation_steps\n","    self.scheduler = scheduler\n","    self.loss_func = loss_func\n","    self.device = device\n","\n","    if scheduler is not None:\n","      for group in self.optimizer.param_groups:\n","        group.setdefault('initial_lr', group['lr'])\n","      self.scheduler.base_lrs([group['initial_lr'] for group in self.optimizer.param_groups])\n","\n","  def adapt(self, inputs, targets, num_adaptation_steps=1, step_size=0.1):\n","    params = None\n","    # Initialize results['inner losses'] (per task)\n","    results = {'inner_losses': np.zeros((num_adaptation_steps, ), dtype=np.float32)}\n","    for step in range(num_adaptation_steps):\n","      logits = self.model(inputs, params=params)\n","      inner_loss = self.loss_func(logits, targets)\n","      results['inner_losses'][step] = inner_loss.item()\n","      if step == 0:\n","        results['accuracy_before'] = compute_accuracy(logits, targets)\n","      self.model.zero_grad()\n","      params = gradient_update_parameters(self.model, inner_loss, step_size=step_size, params=params)\n","    return params, results\n","\n","  # per batch\n","  def get_outer_loss(self, batch):\n","    if 'test' not in batch:\n","      raise RuntimeError('The batch does not contain any test dataset')\n","\n","    _,  test_targets = batch['test']\n","    num_tasks = test_targets.size(0) # Here, 16\n","\n","    # Initialize results\n","    results = {\n","        'num_tasks': num_tasks,\n","        'inner_losses': np.zeros((self.num_adaptation_steps, num_tasks), dtype=np.float32),\n","        'outer_losses': np.zeros((num_tasks, ), dtype=np.float32),\n","        'mean_outer_loss': 0.,\n","        'accuracies_before': np.zeros((num_tasks, ), dtype=np.float32), # Before adaptation\n","        'accuracies_after': np.zeros((num_tasks, ), dtype=np.float32) # After adaptation\n","    }\n","\n","    mean_outer_loss = torch.tensor(0., device=self.device)\n","\n","    for task_id, (train_inputs, train_targets, test_inputs, test_targets) in enumerate(zip(*batch['train'], *batch['test'])):\n","      # adaptation_results(per task): {'inner_losses': (num_adaptation_steps, ), 'accuracy_before': compute_accuracy(logits, targets)} \n","      params, adaptation_results = self.adapt(train_inputs, train_targets, \n","                                              num_adaptation_steps=self.num_adaptation_steps, step_size=self.step_size)\n","      results['inner_losses'][:, task_id] = adaptation_results['inner_losses']\n","      results['accuracies_before'][task_id] = adaptation_results['accuracy_before']\n","      with torch.set_grad_enabled(self.model.training):\n","        test_logits = self.model(test_inputs, params=params) # param is updated with (train_inputs, train_targets)\n","        outer_loss = self.loss_func(test_logits, test_targets)\n","        results['outer_losses'][task_id] = outer_loss.item()\n","        mean_outer_loss += outer_loss\n","      results['accuracies_after'][task_id] = compute_accuracy(test_logits, test_targets)\n","    \n","    mean_outer_loss.div_(num_tasks)\n","    results['mean_outer_loss'] = mean_outer_loss.item()\n","\n","    return mean_outer_loss, results\n","\n","  def train_iter(self, dataloader, max_batches=500):\n","    num_batches = 0\n","    self.model.train()\n","    model.to(device=self.device)\n","    while num_batches < max_batches:\n","      for batch in dataloader:\n","        if num_batches >= max_batches: break\n","\n","        if self.scheduler is not None:\n","          self.scheduler.step(epoch=num_batches)\n","\n","        self.optimizer.zero_grad()\n","        batch = tensors_to_device(batch, device=self.device)\n","        outer_loss, results = self.get_outer_loss(batch)\n","        yield results\n","\n","        outer_loss.backward()\n","        self.optimizer.step()\n","\n","        num_batches += 1\n","\n","  def train(self, dataloader, max_batches=500, verbose=True, **kwargs):\n","    with tqdm(total=max_batches, disable=not verbose, **kwargs) as pbar:\n","      for results in self.train_iter(dataloader, max_batches=max_batches):\n","        pbar.update(1)\n","        postfix = {'loss': '{0:.4f}'.format(results['mean_outer_loss'])}\n","        if 'accuracies_after' in results:\n","          postfix['accuracy'] = '{0:.4f}'.format(np.mean(results['accuracies_after']))\n","        pbar.set_postfix(**postfix)\n","  \n","  def evaluate_iter(self, dataloader, max_batches=500):\n","    num_batches = 0\n","    self.model.eval()\n","    model.to(device=self.device)\n","    while num_batches < max_batches:\n","      for batch in dataloader:\n","        if num_batches >= max_batches: break\n","\n","        batch = tensors_to_device(batch, device=self.device)\n","        _, results = self.get_outer_loss(batch)\n","        yield results\n","\n","        num_batches += 1\n","  \n","  def evaluate(self, dataloader, max_batches=500, verbose=True, **kwargs):\n","    mean_outer_loss, mean_accuracy, count = 0., 0., 0\n","    with tqdm(total=max_batches, disable=not verbose, **kwargs) as pbar:\n","      for results in self.evaluate_iter(dataloader, max_batches=max_batches):\n","        pbar.update(1)\n","        count += 1\n","        mean_outer_loss += (results['mean_outer_loss'] - mean_outer_loss) / count # Running average for the batches\n","        postfix = {'loss': '{0:.4f}'.format(mean_outer_loss)}\n","        if 'accuracies_after' in results:\n","          mean_accuracy += (np.mean(results['accuracies_after']) - mean_accuracy) / count # Running average for the batches\n","          postfix['accuracy'] = '{0:.4f}'.format(mean_accuracy)\n","        pbar.set_postfix(**postfix)\n","    \n","    mean_results = {'mean_outer_loss': mean_outer_loss}\n","    if 'accuracies_after' in results:\n","      mean_results['accuracies_after'] = mean_accuracy\n","      \n","    return mean_results"],"metadata":{"id":"saimerPyNV53"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"k4Sp4UbO21WR"}},{"cell_type":"code","source":["import torch\n","import math\n","import os\n","import time\n","import json\n","import logging"],"metadata":{"id":"q0RzjEMI28FR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxYBh92M28HU","executionInfo":{"status":"ok","timestamp":1646446883586,"user_tz":-540,"elapsed":413,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"5d63b07b-77f6-4240-d134-5156d95776eb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["output_folder = '/path/to/data'\n","if not os.path.exists(output_folder):\n","  os.makedirs(output_folder)\n","  logging.debug('Creating output_folder `{0}`'.format(output_folder))\n","\n","  folder = os.path.join(output_folder, time.strftime('%Y-%m-%d_%H%M%S'))\n","  os.makedirs(folder)\n","  logging.debug('Creating folder `{0}`'.format(folder))\n","\n","  folder = os.path.abspath(folder)\n","  model_path = os.path.abspath(os.path.join(folder, 'model.th'))\n","\n","# For Omniglot dataset (train & val)\n","batch_size=16\n","num_workers=4\n","\n","meta_train_dataset = Omniglot(root=\"data\",\n","                  num_classes_per_task=5,\n","                  transform=Compose([Resize(28), ToTensor()]),\n","                  target_transform=Categorical(num_classes=5),\n","                  class_augmentations=[Rotation([90, 180, 270])],\n","                  meta_train=True,\n","                  download=True)\n","meta_train_dataset = ClassSplitter(meta_train_dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n","meta_train_dataloader = BatchMetaDataLoader(meta_train_dataset, batch_size=batch_size, num_workers=num_workers)\n","\n","meta_val_dataset = Omniglot(root=\"data\",\n","                  num_classes_per_task=5,\n","                  transform=Compose([Resize(28), ToTensor()]),\n","                  target_transform=Categorical(num_classes=5),\n","                  class_augmentations=[Rotation([90, 180, 270])],\n","                  meta_val=True)\n","meta_val_dataset = ClassSplitter(meta_val_dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n","meta_val_dataloader = BatchMetaDataLoader(meta_val_dataset, batch_size=batch_size, num_workers=num_workers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5YdkVTFt28NT","executionInfo":{"status":"ok","timestamp":1646446883586,"user_tz":-540,"elapsed":11,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"9be77867-55d5-4160-ef67-593d7bfa77b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["model = ModelConvOmniglot(num_ways=5, hidden_size=64)\n","num_batches=100\n","loss_func = F.cross_entropy\n","meta_lr = 0.001\n","meta_optimizer = torch.optim.Adam(model.parameters(), lr=meta_lr)\n","metalearner = MAML(model=model, optimizer=meta_optimizer, step_size=0.1, num_adaptation_steps=1, \n","                   scheduler=None, loss_func=loss_func, device=device)"],"metadata":{"id":"TD29fjZw6i4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to(device)\n","best_value = None\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","  metalearner.train(dataloader=meta_train_dataloader, max_batches=num_batches, verbose=True)\n","  results = metalearner.evaluate(dataloader=meta_val_dataloader, max_batches=num_batches, verbose=True)\n","  \n","  if 'accuracies_after' in results:\n","    if (best_value is None) or (best_value < results['accuracies_after']):\n","      best_value = results['accuracies_after']\n","      save_model = True\n","  elif (best_value is None) or (best_value > results['mean_outer_loss']):\n","    best_value = results['mean_outer_loss']\n","    save_model = True\n","  else:\n","    save_model = False\n","\n","  if save_model and (output_folder is not None):\n","    with open(model_path, 'wb') as f:\n","      torch.save(model, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKh7MXnC9Oyc","executionInfo":{"status":"ok","timestamp":1646448680282,"user_tz":-540,"elapsed":1796699,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"55b18939-ee91-41dd-c5f9-13dbab1fbbdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:40<00:00,  1.01s/it, accuracy=0.9617, loss=0.1394]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:18<00:00,  1.28it/s, accuracy=0.9627, loss=0.1609]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:40<00:00,  1.01s/it, accuracy=0.9842, loss=0.0737]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:16<00:00,  1.31it/s, accuracy=0.9719, loss=0.1086]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.02s/it, accuracy=0.9875, loss=0.0686]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:18<00:00,  1.27it/s, accuracy=0.9813, loss=0.0742]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.01s/it, accuracy=0.9875, loss=0.0503]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:16<00:00,  1.31it/s, accuracy=0.9850, loss=0.0554]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:40<00:00,  1.01s/it, accuracy=0.9867, loss=0.0465]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:16<00:00,  1.30it/s, accuracy=0.9878, loss=0.0459]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.01s/it, accuracy=0.9908, loss=0.0361]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.29it/s, accuracy=0.9846, loss=0.0553]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.02s/it, accuracy=0.9933, loss=0.0248]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:16<00:00,  1.31it/s, accuracy=0.9906, loss=0.0363]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:41<00:00,  1.02s/it, accuracy=0.9950, loss=0.0183]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:21<00:00,  1.23it/s, accuracy=0.9893, loss=0.0369]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.02s/it, accuracy=0.9942, loss=0.0272]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.29it/s, accuracy=0.9908, loss=0.0337]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:42<00:00,  1.03s/it, accuracy=0.9933, loss=0.0303]\n","  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:974: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","100%|██████████| 100/100 [01:17<00:00,  1.28it/s, accuracy=0.9912, loss=0.0303]\n"]}]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"2IL8ybPWJ8bq"}},{"cell_type":"code","source":["with open(model_path, 'rb') as f:\n","  best_model = torch.load(f, map_location=device)"],"metadata":{"id":"uhimu8aQJ90K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meta_test_dataset = Omniglot(root=\"data\",\n","                  num_classes_per_task=5,\n","                  transform=Compose([Resize(28), ToTensor()]),\n","                  target_transform=Categorical(num_classes=5),\n","                  meta_test=True)\n","meta_test_dataset = ClassSplitter(meta_test_dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n","meta_test_dataloader = BatchMetaDataLoader(meta_test_dataset, batch_size=batch_size, num_workers=num_workers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"liQSMGsbJ92e","executionInfo":{"status":"ok","timestamp":1646448680283,"user_tz":-540,"elapsed":19,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"ddac40a0-c691-4656-e551-2cb76219002c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["metalearner_test = MAML(model=best_model, optimizer=None, step_size=0.1, num_adaptation_steps=1,\n","                   scheduler=None, loss_func=loss_func, device=device)\n","results_test = metalearner_test.evaluate(dataloader=meta_test_dataloader, max_batches=num_batches, verbose=True)\n","\n","# Save results\n","dirname = os.path.dirname(model_path)\n","with open(os.path.join(dirname, 'results_test.json'), 'w') as f:\n","    json.dump(results_test, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTW3bZZmJ941","executionInfo":{"status":"ok","timestamp":1646448747850,"user_tz":-540,"elapsed":67574,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"30b7e08e-a702-42b7-fc18-ce2bbf832ba8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","100%|██████████| 100/100 [01:07<00:00,  1.48it/s, accuracy=0.9869, loss=0.0458]\n"]}]}]}